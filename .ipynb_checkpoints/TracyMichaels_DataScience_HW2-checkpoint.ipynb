{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 2: Data Prep\n",
    "In this homework assignment, you will continue your exploration of the [SWAN-SF Dataset](https://doi.org/10.7910/DVN/EBCFKM), described in the paper found [here](https://doi.org/10.1038/s41597-020-0548-x).\n",
    "\n",
    "\n",
    "This assignment will have you explore the cardinalities, number of missing values, detect outliers, handle missing values and outliers, and create data quality report for original and cleaned dataset. Additionally, you will be asked to provide documentation for your functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the Data\n",
    "\n",
    "This assignment will only be using [Partition 1](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/BMXYCB). Recall that in Homework 1 we started to construct the analytics base table for our [SWAN-SF Dataset](https://doi.org/10.7910/DVN/EBCFKM). In that assignment, we read the data from the two subdirectories, __FL__ and __NF__, of the __partition1__ direcotry. These two subdirectories represented the two classes of our target feature in the solar flare prediction problem we will be attempting to solve this semester. We then processed these samples of multivariate time series to construct descriptive features for each sample, and then placed them into our analytics base table.\n",
    "\n",
    "In this assignment, you will be utilizing a set of extracted descriptive features much like what you were asked to construct in Homework 1. However, this dataset contains many more extracted features than you were asked to compute for Homework 1 (>800), so we need to explore the data to find data quality issues and identify ways to address these issues. Below are links to the full extracted feature dataset for partition 1 and a toy dataset to use for testing you functions.\n",
    "\n",
    "__Note:__ Since the full dataset, and multiple copies of partially processed intermediary results, tend to take up a bit of space, you can use the toy dataset to implement and test your code. You may need to edit the data to fully test each of the requirements, but that is left as an exercise for the student. The full partition dataset is only included for those who wish to work with it once they have their code implemented. \n",
    "\n",
    "- [Full Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/partition1ExtractedFeatures.csv)\n",
    "- [Toy Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_partition1ExtractedFeatures.csv)\n",
    "\n",
    "Now that you have the extracted features csv files, you will load that data into a Pandas DataFrame using the [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame \n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'C:\\Users\\Tracy\\DSWorkspace\\hw2\\data\\MVTS'\n",
    "# toy\n",
    "# data_file = 'toy_partition1ExtractedFeatures.csv'\n",
    "#full set\n",
    "data_file = 'partition1ExtractedFeatures.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt = pd.read_csv(os.path.join(data_dir, data_file), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lab</th>\n",
       "      <th>st</th>\n",
       "      <th>et</th>\n",
       "      <th>TOTUSJH_min</th>\n",
       "      <th>TOTUSJH_max</th>\n",
       "      <th>TOTUSJH_median</th>\n",
       "      <th>TOTUSJH_mean</th>\n",
       "      <th>TOTUSJH_stddev</th>\n",
       "      <th>TOTUSJH_var</th>\n",
       "      <th>...</th>\n",
       "      <th>R_VALUE_gderivative_kurtosis</th>\n",
       "      <th>R_VALUE_linear_weighted_average</th>\n",
       "      <th>R_VALUE_quadratic_weighted_average</th>\n",
       "      <th>R_VALUE_average_absolute_change</th>\n",
       "      <th>R_VALUE_average_absolute_derivative_change</th>\n",
       "      <th>R_VALUE_last_value</th>\n",
       "      <th>R_VALUE_slope_of_longest_mono_increase</th>\n",
       "      <th>R_VALUE_slope_of_longest_mono_decrease</th>\n",
       "      <th>R_VALUE_avg_mono_increase_slope</th>\n",
       "      <th>R_VALUE_avg_mono_decrease_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>514</td>\n",
       "      <td>C</td>\n",
       "      <td>2011-04-28T23:12:00</td>\n",
       "      <td>2011-04-29T11:00:00</td>\n",
       "      <td>1654.553362</td>\n",
       "      <td>1815.727348</td>\n",
       "      <td>1725.894296</td>\n",
       "      <td>1732.208184</td>\n",
       "      <td>44.055204</td>\n",
       "      <td>1940.861005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.810154</td>\n",
       "      <td>4.478583</td>\n",
       "      <td>4.496122</td>\n",
       "      <td>0.024698</td>\n",
       "      <td>0.038708</td>\n",
       "      <td>4.574968</td>\n",
       "      <td>0.032692</td>\n",
       "      <td>-0.000901</td>\n",
       "      <td>0.016224</td>\n",
       "      <td>-0.014214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>NF</td>\n",
       "      <td>2010-08-04T06:36:00</td>\n",
       "      <td>2010-08-04T18:24:00</td>\n",
       "      <td>38.257743</td>\n",
       "      <td>104.732191</td>\n",
       "      <td>48.946497</td>\n",
       "      <td>54.105280</td>\n",
       "      <td>15.635504</td>\n",
       "      <td>244.468972</td>\n",
       "      <td>...</td>\n",
       "      <td>21.885734</td>\n",
       "      <td>1.288584</td>\n",
       "      <td>1.696440</td>\n",
       "      <td>0.084950</td>\n",
       "      <td>0.141598</td>\n",
       "      <td>2.817515</td>\n",
       "      <td>0.704938</td>\n",
       "      <td>-0.053054</td>\n",
       "      <td>0.242427</td>\n",
       "      <td>-0.090127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>798</td>\n",
       "      <td>C</td>\n",
       "      <td>2011-08-21T05:12:00</td>\n",
       "      <td>2011-08-21T17:00:00</td>\n",
       "      <td>807.927312</td>\n",
       "      <td>927.994392</td>\n",
       "      <td>860.664804</td>\n",
       "      <td>861.078366</td>\n",
       "      <td>35.013665</td>\n",
       "      <td>1225.956749</td>\n",
       "      <td>...</td>\n",
       "      <td>2.698058</td>\n",
       "      <td>4.084092</td>\n",
       "      <td>4.132907</td>\n",
       "      <td>0.052272</td>\n",
       "      <td>0.081651</td>\n",
       "      <td>4.201446</td>\n",
       "      <td>0.110665</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.032708</td>\n",
       "      <td>-0.031965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1372</td>\n",
       "      <td>NF</td>\n",
       "      <td>2012-02-02T11:48:00</td>\n",
       "      <td>2012-02-02T23:36:00</td>\n",
       "      <td>97.702918</td>\n",
       "      <td>133.588624</td>\n",
       "      <td>110.826583</td>\n",
       "      <td>112.290552</td>\n",
       "      <td>8.399740</td>\n",
       "      <td>70.555625</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>475</td>\n",
       "      <td>NF</td>\n",
       "      <td>2011-04-09T01:24:00</td>\n",
       "      <td>2011-04-09T13:12:00</td>\n",
       "      <td>287.054395</td>\n",
       "      <td>357.415061</td>\n",
       "      <td>317.865356</td>\n",
       "      <td>321.454795</td>\n",
       "      <td>14.461723</td>\n",
       "      <td>209.141437</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 820 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id lab                   st                   et  TOTUSJH_min  \\\n",
       "0   514   C  2011-04-28T23:12:00  2011-04-29T11:00:00  1654.553362   \n",
       "1   107  NF  2010-08-04T06:36:00  2010-08-04T18:24:00    38.257743   \n",
       "2   798   C  2011-08-21T05:12:00  2011-08-21T17:00:00   807.927312   \n",
       "3  1372  NF  2012-02-02T11:48:00  2012-02-02T23:36:00    97.702918   \n",
       "4   475  NF  2011-04-09T01:24:00  2011-04-09T13:12:00   287.054395   \n",
       "\n",
       "   TOTUSJH_max  TOTUSJH_median  TOTUSJH_mean  TOTUSJH_stddev  TOTUSJH_var  \\\n",
       "0  1815.727348     1725.894296   1732.208184       44.055204  1940.861005   \n",
       "1   104.732191       48.946497     54.105280       15.635504   244.468972   \n",
       "2   927.994392      860.664804    861.078366       35.013665  1225.956749   \n",
       "3   133.588624      110.826583    112.290552        8.399740    70.555625   \n",
       "4   357.415061      317.865356    321.454795       14.461723   209.141437   \n",
       "\n",
       "   ...  R_VALUE_gderivative_kurtosis  R_VALUE_linear_weighted_average  \\\n",
       "0  ...                     -0.810154                         4.478583   \n",
       "1  ...                     21.885734                         1.288584   \n",
       "2  ...                      2.698058                         4.084092   \n",
       "3  ...                     -3.000000                         0.000000   \n",
       "4  ...                     -3.000000                         0.000000   \n",
       "\n",
       "   R_VALUE_quadratic_weighted_average  R_VALUE_average_absolute_change  \\\n",
       "0                            4.496122                         0.024698   \n",
       "1                            1.696440                         0.084950   \n",
       "2                            4.132907                         0.052272   \n",
       "3                            0.000000                         0.000000   \n",
       "4                            0.000000                         0.000000   \n",
       "\n",
       "   R_VALUE_average_absolute_derivative_change  R_VALUE_last_value  \\\n",
       "0                                    0.038708            4.574968   \n",
       "1                                    0.141598            2.817515   \n",
       "2                                    0.081651            4.201446   \n",
       "3                                    0.000000            0.000000   \n",
       "4                                    0.000000            0.000000   \n",
       "\n",
       "   R_VALUE_slope_of_longest_mono_increase  \\\n",
       "0                                0.032692   \n",
       "1                                0.704938   \n",
       "2                                0.110665   \n",
       "3                                0.000000   \n",
       "4                                0.000000   \n",
       "\n",
       "   R_VALUE_slope_of_longest_mono_decrease  R_VALUE_avg_mono_increase_slope  \\\n",
       "0                               -0.000901                         0.016224   \n",
       "1                               -0.053054                         0.242427   \n",
       "2                               -0.004080                         0.032708   \n",
       "3                                0.000000                         0.000000   \n",
       "4                                0.000000                         0.000000   \n",
       "\n",
       "   R_VALUE_avg_mono_decrease_slope  \n",
       "0                        -0.014214  \n",
       "1                        -0.090127  \n",
       "2                        -0.031965  \n",
       "3                         0.000000  \n",
       "4                         0.000000  \n",
       "\n",
       "[5 rows x 820 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making sure it loaded\n",
    "abt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 (20 points)\n",
    "\n",
    "Write a function to extract the various pieces of a data quality report, for a specific attribute, and return a dataframe with this information.\n",
    "\n",
    " * 'Feature Name': Contains the time series statistical feature name\n",
    " \n",
    " * 'Cardinality': Contains the count of unique values for the feature\n",
    "            \n",
    " * 'Non-null Count': Contains the number of non-null entries for the feature\n",
    "            \n",
    " * 'Null Count': Contains the number of null or missing entries for the feature\n",
    "            \n",
    " * 'Min': Contains the minimum value of the feature (Without considering the null or nan value)\n",
    " \n",
    " * '25th': Contains the first quartile (25%) of the feature values (Without considering the null/nan value)\n",
    " \n",
    " * 'Mean': Contains the mean of the feature values (Without considering the null/nan value)\n",
    " \n",
    " * '50th': Contains the median of the feature values (Without considering the null/nan value)\n",
    "            \n",
    " * '75th': Contains the third quartile (75%) of the feature values (Without considering the null/nan value)\n",
    " \n",
    " * 'Max': Contains the maximum value of the feature (Without considering the null/nan value),\n",
    "            \n",
    " * 'Std. Dev': Contains the standard deviation of the feature (Without considering the null/nan value)\n",
    " \n",
    "In addition to the values above, you should identify the number of upper and lower outliers using the $val < Q1-1.5IQR$ and $val > Q3+1.5IQR$ outlier identification method. These added features should be called `Outlier Count Low` and `Outliers Count High` respectively.\n",
    "\n",
    "\n",
    " \n",
    " Some useful functions for this can be found at:\n",
    " \n",
    " * [Numpy.percentile](https://numpy.org/doc/stable/reference/generated/numpy.percentile.html)\n",
    " \n",
    " * [pandas.isna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isna.html)\n",
    " \n",
    " * [Numpy.mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)\n",
    " \n",
    " * [Numpy.std](https://numpy.org/doc/stable/reference/generated/numpy.std.html)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_summary_for(feature_name:str, data:DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate a summary of a specified feature from a given DataFrame\n",
    "    Items to be calculated (becomes column names of output):\n",
    "        'Feature Name': Contains the time series statistical feature name\n",
    "        'Cardinality': Contains the count of unique values for the feature\n",
    "        'Non-null Count': Contains the number of non-null entries for the feature\n",
    "        'Null Count': Contains the number of null or missing entries for the feature\n",
    "        'Min': Contains the minimum value of the feature (Without considering the null or nan value)\n",
    "        '25th': Contains the first quartile (25%) of the feature values (Without considering the null/nan value)\n",
    "        'Mean': Contains the mean of the feature values (Without considering the null/nan value)\n",
    "        '50th': Contains the median of the feature values (Without considering the null/nan value)\n",
    "        '75th': Contains the third quartile (75%) of the feature values (Without considering the null/nan value)\n",
    "        'Max': Contains the maximum value of the feature (Without considering the null/nan value),\n",
    "        'Std. Dev': Contains the standard deviation of the feature (Without considering the null/nan value)\n",
    "        \n",
    "    Parameters\n",
    "    -----------\n",
    "    feature_name : str\n",
    "        name of the feature on which summary will be calculated\n",
    "        feature name must be present in passed DataFeame\n",
    "    \n",
    "    data : DataFrame\n",
    "        a DataFrame containing features and data to be calculated\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        DataFrame containing columns with summary feature names and a row with calculated values for each column\n",
    "    \"\"\"\n",
    "    summary_feature_names = ['Feature Name', 'Cardinality', 'Non-null Count', 'Null Count', 'Min', '25th', 'Mean', \n",
    "                             '50th', '75th', 'Max', 'Std. Dev','Outlier Count Low', 'Outlier Count High']\n",
    "    # Your answer to Q1 goes here!\n",
    "    # Make sure to return a DataFrame with the features specified.\n",
    "    \n",
    "    result_ls = []\n",
    "    \n",
    "    # 5 num summary\n",
    "    min = data[feature_name].min()\n",
    "    max = data[feature_name].max()\n",
    "    quarts = np.percentile(data[feature_name].to_numpy(), [25, 50, 75], interpolation='midpoint')\n",
    "    \n",
    "    # num outliers\n",
    "    iqr = quarts[2] - quarts[0]\n",
    "    iqr_e = 1.5 * iqr\n",
    "    out_min_ct = len([i for i in data[feature_name] if i < (quarts[0] - iqr_e)])\n",
    "    out_max_ct = len([i for i in data[feature_name] if i > (quarts[2] + iqr_e)])\n",
    "    \n",
    "    # construct summary\n",
    "    # name\n",
    "    result_ls.append(feature_name)\n",
    "    # card\n",
    "    result_ls.append(len(data[feature_name].unique()))\n",
    "    # non-null ct\n",
    "    result_ls.append(data[feature_name].notna().sum())\n",
    "    # is null ct\n",
    "    result_ls.append(data[feature_name].isna().sum())\n",
    "    # min\n",
    "    result_ls.append(min)\n",
    "    # 25th\n",
    "    result_ls.append(quarts[0])\n",
    "    # mean\n",
    "    result_ls.append(np.mean(data[feature_name].to_numpy()))\n",
    "    # 50th\n",
    "    result_ls.append(quarts[1])\n",
    "    # 75th\n",
    "    result_ls.append(quarts[2])\n",
    "    # max\n",
    "    result_ls.append(max)\n",
    "    #std dev\n",
    "    result_ls.append(np.std(data[feature_name].to_numpy()))\n",
    "    # out low ct\n",
    "    result_ls.append(out_min_ct)\n",
    "    # out high ct\n",
    "    result_ls.append(out_max_ct)\n",
    "    \n",
    "    \n",
    "    # return DataFrame of calc summary\n",
    "    return pd.DataFrame([result_ls], columns = summary_feature_names)    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Cardinality</th>\n",
       "      <th>Non-null Count</th>\n",
       "      <th>Null Count</th>\n",
       "      <th>Min</th>\n",
       "      <th>25th</th>\n",
       "      <th>Mean</th>\n",
       "      <th>50th</th>\n",
       "      <th>75th</th>\n",
       "      <th>Max</th>\n",
       "      <th>Std. Dev</th>\n",
       "      <th>Outlier Count Low</th>\n",
       "      <th>Outlier Count High</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TOTUSJH_min</td>\n",
       "      <td>30165</td>\n",
       "      <td>73492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.75602</td>\n",
       "      <td>396.697968</td>\n",
       "      <td>110.011893</td>\n",
       "      <td>418.614528</td>\n",
       "      <td>5680.232811</td>\n",
       "      <td>694.293786</td>\n",
       "      <td>0</td>\n",
       "      <td>9014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name  Cardinality  Non-null Count  Null Count  Min      25th  \\\n",
       "0  TOTUSJH_min        30165           73492           0  0.0  29.75602   \n",
       "\n",
       "         Mean        50th        75th          Max    Std. Dev  \\\n",
       "0  396.697968  110.011893  418.614528  5680.232811  694.293786   \n",
       "\n",
       "   Outlier Count Low  Outlier Count High  \n",
       "0                  0                9014  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_TOTUSJH_min = calc_summary_for('TOTUSJH_min', abt)\n",
    "# check output\n",
    "display(summary_TOTUSJH_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (20 points)\n",
    "Using what you produced to answere Q1, you should now write a function to construct the data quality report for all of the numerical features of our dataset.  You should loop over all of the features in the analytics base table represented by the input feature dataset files from partition 1, with the exception of the first column (this is the index column if you read the file correctly), and the `id`, `lab`, `st`, and `et` columns.  \n",
    "\n",
    "Your output from this function will be a DataFrame that has 1 row for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_quality_report(data:DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    calculates the summary for each feature in a DataFrame using the calc_summary_for() function\n",
    "    Excludes features ['id', 'lab', 'st', 'et']\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        a DataFrame containing features and data to calculate a summary\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        a DataFrame containing columns of calculated summary items and rows of features from input data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    excluded_columns = ['id', 'lab', 'st', 'et']\n",
    "    \n",
    "    summary_feature_names = ['Feature Name', 'Cardinality', 'Non-null Count', 'Null Count', 'Min', '25th', 'Mean', \n",
    "                             '50th', '75th', 'Max', 'Std. Dev','Outlier Count Low', 'Outlier Count High']\n",
    "    \n",
    "    # Your answer to Q2 goes here!\n",
    "    # Make sure to return a DataFrame with the features specified.    \n",
    "    result_ls = [calc_summary_for(i, data).values.flatten().tolist() for i in data.columns if i not in excluded_columns]\n",
    "    return pd.DataFrame(result_ls, columns = summary_feature_names)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(816, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Cardinality</th>\n",
       "      <th>Non-null Count</th>\n",
       "      <th>Null Count</th>\n",
       "      <th>Min</th>\n",
       "      <th>25th</th>\n",
       "      <th>Mean</th>\n",
       "      <th>50th</th>\n",
       "      <th>75th</th>\n",
       "      <th>Max</th>\n",
       "      <th>Std. Dev</th>\n",
       "      <th>Outlier Count Low</th>\n",
       "      <th>Outlier Count High</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TOTUSJH_min</td>\n",
       "      <td>30165</td>\n",
       "      <td>73492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.756020</td>\n",
       "      <td>396.697968</td>\n",
       "      <td>110.011893</td>\n",
       "      <td>418.614528</td>\n",
       "      <td>5680.232811</td>\n",
       "      <td>694.293786</td>\n",
       "      <td>0</td>\n",
       "      <td>9014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TOTUSJH_max</td>\n",
       "      <td>24142</td>\n",
       "      <td>73492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.783099</td>\n",
       "      <td>53.115131</td>\n",
       "      <td>492.649021</td>\n",
       "      <td>162.797367</td>\n",
       "      <td>541.875882</td>\n",
       "      <td>6078.237599</td>\n",
       "      <td>801.078303</td>\n",
       "      <td>0</td>\n",
       "      <td>8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TOTUSJH_median</td>\n",
       "      <td>62001</td>\n",
       "      <td>73492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.726051</td>\n",
       "      <td>439.417610</td>\n",
       "      <td>133.981693</td>\n",
       "      <td>472.725349</td>\n",
       "      <td>5895.663839</td>\n",
       "      <td>741.152887</td>\n",
       "      <td>0</td>\n",
       "      <td>8799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TOTUSJH_mean</td>\n",
       "      <td>73492</td>\n",
       "      <td>73492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100756</td>\n",
       "      <td>40.832741</td>\n",
       "      <td>440.333385</td>\n",
       "      <td>134.524140</td>\n",
       "      <td>474.264394</td>\n",
       "      <td>5877.701432</td>\n",
       "      <td>741.767409</td>\n",
       "      <td>0</td>\n",
       "      <td>8783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TOTUSJH_stddev</td>\n",
       "      <td>73492</td>\n",
       "      <td>73492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>4.926582</td>\n",
       "      <td>25.019193</td>\n",
       "      <td>11.410790</td>\n",
       "      <td>30.408896</td>\n",
       "      <td>618.834925</td>\n",
       "      <td>35.130481</td>\n",
       "      <td>0</td>\n",
       "      <td>6684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature Name  Cardinality  Non-null Count  Null Count       Min  \\\n",
       "0     TOTUSJH_min        30165           73492           0  0.000000   \n",
       "1     TOTUSJH_max        24142           73492           0  0.783099   \n",
       "2  TOTUSJH_median        62001           73492           0  0.000000   \n",
       "3    TOTUSJH_mean        73492           73492           0  0.100756   \n",
       "4  TOTUSJH_stddev        73492           73492           0  0.176471   \n",
       "\n",
       "        25th        Mean        50th        75th          Max    Std. Dev  \\\n",
       "0  29.756020  396.697968  110.011893  418.614528  5680.232811  694.293786   \n",
       "1  53.115131  492.649021  162.797367  541.875882  6078.237599  801.078303   \n",
       "2  40.726051  439.417610  133.981693  472.725349  5895.663839  741.152887   \n",
       "3  40.832741  440.333385  134.524140  474.264394  5877.701432  741.767409   \n",
       "4   4.926582   25.019193   11.410790   30.408896   618.834925   35.130481   \n",
       "\n",
       "   Outlier Count Low  Outlier Count High  \n",
       "0                  0                9014  \n",
       "1                  0                8540  \n",
       "2                  0                8799  \n",
       "3                  0                8783  \n",
       "4                  0                6684  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table = construct_quality_report(abt)\n",
    "print(summary_table.shape)  # checking the dimensionality is often a useful test.\n",
    "summary_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (20 points)\n",
    "#### Drop features with improper cardinality:\n",
    "Using the quality report summary table that is returned from the function you wrote for Q2, we are now going to investigate our data. For this, you should use the table returned for the [Full Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/partition1ExtractedFeatures.csv) and not the toy dataset I provided for testing.\n",
    "\n",
    "Since we are using real valued features, a majority of them shall have a cardinality close to the sample count. So, for this question, you are to write a function that takes in the summary table and the input dataset DataFrame, and drops the feature that have a cardinality less than 10. This feature should be dropped from both the data quality report summary table and from the actual input dataset DataFrame.\n",
    "\n",
    "A useful method for this operation is:\n",
    "\n",
    "* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) (Make sure to use the inplace option otherwise it returns a copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_low_card_data(summary_table:DataFrame, data:DataFrame) -> None:\n",
    "    # Your answer to Q3 goes here! \n",
    "    \"\"\"\n",
    "    Drop features from DataFrame that have a cardinality less than 10\n",
    "    Drops are performed in place so returns None\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    summary_table : DataFrame\n",
    "        DataFrame containing summary values of a dataset\n",
    "        must contain 'Cardinality' and 'Feature Name' features\n",
    "    \n",
    "    data : DataFrame\n",
    "        input dataset, an analytics base table for which summary_table was calculated\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"    \n",
    "       \n",
    "    # TODO - remove\n",
    "    # while this does work, it also drops the features that were excluded in previous question from the abt\n",
    "    # yeilds shape summary_table.shape == (801, 13) and len(abt.columns) == 801 (should be 805)\n",
    "#     summary_table.drop([i for i in summary_table.index if summary_table['Cardinality'][i] < 10], inplace=True)\n",
    "#     data.drop(list(set(data.columns.tolist()) - set(summary_table['Feature Name'].tolist())), 1, inplace=True)\n",
    "\n",
    "    # TODO - remove          \n",
    "    # yeilds shape summary_table.shape == (816, 13) and len(abt.columns) == 820 (???????????)\n",
    "    # not sure why there are 15 extra features...\n",
    "    # also this is just messy anyway, using append in list comp\n",
    "#     drop_i = []\n",
    "#     drop_n = []\n",
    "#     (drop_i.append(i) and drop_n.append(summary_table['Feature Name'][i])\\\n",
    "#     for i in summary_table.index if summary_table['Cardinality'][i] < 10)\n",
    "\n",
    "#     data.drop(drop_n, inplace=True)    \n",
    "#     summary_table.drop(drop_i, inplace=True)        \n",
    "\n",
    "\n",
    "    # yeilds shape summary_table.shape == (801, 13) and len(abt.columns) == 805 (expected)\n",
    "    for i in summary_table.index:\n",
    "        if summary_table['Cardinality'][i] < 10:\n",
    "            # order of drops is important here***********\n",
    "            data.drop(summary_table['Feature Name'][i],1, inplace=True)\n",
    "            summary_table.drop(i, inplace=True)    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 13)\n"
     ]
    }
   ],
   "source": [
    "drop_low_card_data(summary_table, abt)\n",
    "print(summary_table.shape)\n",
    "#print(len(abt.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 (20 points)\n",
    "#### Drop features with excessive NaN\n",
    "\n",
    "Again, using the quality report summary table that is returned from the function you wrote for Q2, we are going to continue investigating our data. For this, you should still be using the table returned for the [Full Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/partition1ExtractedFeatures.csv) and not the toy dataset I provided for testing.\n",
    "\n",
    "Like the features that were dropped for Q3, some of the extracted features don't work on all of the variates of the input multi-variate time series samples very well.  So, some of these features return an excessive number of `NaN` values.  These are not verry useful features, so we want to get rid of them before we continue. To do this, you are to write a function that takes in the summary table and the input dataset DataFrame, and drops the features that have **more than 1%** of the entries as null/nan values. Again, these features should be dropped from both the data quality report summary table and from the actual input dataset DataFrame.\n",
    "\n",
    "As in Q3, a useful method for this operation is:\n",
    "\n",
    "* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) (Make sure to use the inplace option otherwise it returns a copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_excessive_nan_data(summary_table:DataFrame, data:DataFrame) -> None:\n",
    "    # Your answer to Q4 goes here!\n",
    "    \"\"\"\n",
    "    Drops features from a DataFrame in which more than 1% of its entries are null/nan values\n",
    "    Drops are performed in place so returns None\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    summary_table : DataFrame\n",
    "        DataFrame containing summary values of a dataset\n",
    "        must contain 'Null Count', 'Non-null Count' and 'Feature Name' features\n",
    "    \n",
    "    data : DataFrame\n",
    "        input dataset, an analytics base table for which summary_table was calculated\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"   \n",
    "    \n",
    "    for i in summary_table.index:\n",
    "        if summary_table['Null Count'][i] / summary_table['Non-null Count'][i] > 0.01:\n",
    "            # order matters here****************\n",
    "            data.drop(summary_table['Feature Name'][i],1, inplace=True)\n",
    "            summary_table.drop(i, inplace=True)            \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(799, 13)\n"
     ]
    }
   ],
   "source": [
    "drop_excessive_nan_data(summary_table, abt)\n",
    "print(summary_table.shape)\n",
    "#print(len(abt.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the data cleaned up a little, save the results of both your summary table and your analytics base table using the [pandas.to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) method. We will want to use these results for the next homework assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = r'C:\\Users\\Tracy\\DSWorkspace\\hw2\\data\\MVTS'\n",
    "out_summary_table_name = 'data_quality_table.csv'\n",
    "out_abt_name = 'cleaned_partition1ExtractedFeatures.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table.to_csv(os.path.join(out_dir, out_summary_table_name))\n",
    "abt.to_csv(os.path.join(out_dir, out_abt_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 (20 points)\n",
    "#### Add docstring to your functions\n",
    "\n",
    "Let's revisit our programming skill while learning Fundamentals of Data Science. \n",
    "\n",
    "Your code is only as valuable as its reusability. Without understandable and legible documentation (which makes maintainability and reusability possible) nobody would like to use your code, let alone to pay for it. ;)\n",
    "\n",
    "If you want to know more about the value of documentation, read [this article](https://www.freecodecamp.org/news/why-documentation-matters-and-why-you-should-include-it-in-your-code-41ef62dd5c2f/). There are even conferences on this topic; see [this website](https://www.writethedocs.org/guide/writing/beginners-guide-to-docs/).\n",
    "\n",
    "In Python, the documentation that is embedded in the code is called **docstring**. In the example below, the \"string\" wrapped in triple quotes is there to tell us all about this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanmean(a, axis=None):\n",
    "    \"\"\"\n",
    "    Compute the arithmetic mean along the specified axis, ignoring NaNs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : array_like\n",
    "        Array containing numbers whose mean is desired. If `a` is not an\n",
    "        array, a conversion is attempted.\n",
    "    axis : {int, tuple of int, None}, optional\n",
    "        Axis or axes along which the means are computed. The default is to compute\n",
    "        the mean of the flattened array.\n",
    "    \"\"\"\n",
    "    # some magic happens here that we don't care about.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is not just a *comment*. If you execute `nanmean` and then call it (as if you want to use it), you can hit `shift+Tab` while your cursor is on the function name, and see how the docstring gets compiled and then pops up. This allows other users to see our description even when they don't have access to our source code. Try it! You can do this with other NumPy and Pandas functions/methods that you've been using.\n",
    "\n",
    "The above example is a simplified version of the method `nanmean` copied from the NumPy library ([here](https://github.com/numpy/numpy/blob/v1.21.0/numpy/lib/nanfunctions.py#L862-L957)). Feel free to check out their complete docstrings.\n",
    "\n",
    "\n",
    "Your last task is to provide docstrings for the 4 methods you've implemented. Simply go back to those cells and modify your functions. Feel free to use the text provided to you (in the assignment descriptions) to enrich your docstrings. Keep in mind that your docstring needs (1) a general description, (2) a short description for each input, and (3) a short description for the output.\n",
    "\n",
    "How to check your docstring? Hit `shift+Tab` and see if the pop-up message is correctly compiled, and make sure your description answers all the questions about your functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
